{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "EXE_MEMORY=\"2G\"\n",
    "DRIVER_MEMORY=\"8G\"\n",
    "spark = SparkSession.builder.appName(\"AWS\").config(\"spark.executor.memory\", EXE_MEMORY).config(\"spark.executor.cores\", \"3\").config(\"spark.driver.memory\", DRIVER_MEMORY).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_rdd = spark.read.json('C:/Users/salon/Documents/project/All_Amazon_Review.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = json_rdd.select('overall','reviewText')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giving ID as idx to each row as reviewerID is not unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---+\n",
      "|overall|          reviewText|idx|\n",
      "+-------+--------------------+---+\n",
      "|    1.0|Alexa is not able...|  1|\n",
      "|    4.0|Alexa works great...|  2|\n",
      "+-------+--------------------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.sql import functions as F\n",
    "dm = dm.withColumn(\"idx\", F.monotonically_increasing_id())\n",
    "windowSpec = W.orderBy(\"idx\")\n",
    "dm.withColumn(\"idx\", F.row_number().over(windowSpec)).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Null Values in reviewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=rp_data.where(dm.reviewText.isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuations \n",
    "\n",
    "For cleaning text, in this we removed punctuations along with trailing and leading spaces. Also lower cased all the alphabets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, trim, col, lower\n",
    "def removePunctuation(column):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "\n",
    "    Note:\n",
    "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
    "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "        punctuation is removed.\n",
    "\n",
    "    Args:\n",
    "        column (Column): A Column containing a sentence.\n",
    "\n",
    "    Returns:\n",
    "        Column: A Column named 'sentence' with clean-up operations applied.\n",
    "    \"\"\"\n",
    "    return trim(lower(regexp_replace(column, '[^\\sa-zA-Z0-9]', ''))).alias('reviewText')\n",
    "\n",
    "rp_data=dm.select(\"IDX\", \"overall\", (removePunctuation(col('reviewText'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "Here we tokenized text, removed all stop words given in \"StopWordRemover\"(list of words is given below). After this we lemmatized the text before stemming and atlast we removed the words of size less or equal than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\salon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.types import *\n",
    "from nltk import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol='reviewText', outputCol='words_token')\n",
    "df_words_token = tokenizer.transform(rp_data).select('idx','overall','words_token')\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol='words_token', outputCol='words_clean')\n",
    "df_words_no_stopw = remover.transform(df_words_token).select('idx', 'overall','words_clean')\n",
    "\n",
    "#lemmatization\n",
    "lemm=WordNetLemmatizer()\n",
    "lemm_udf=udf(lambda tokens:[lemm.lemmatize(token) for token in tokens], ArrayType(StringType()))\n",
    "df_lemm = df_words_no_stopw.withColumn(\"lemmi\", lemm_udf(\"words_clean\")).select('IDX',\"overall\", 'lemmi')\n",
    "\n",
    "# Stem text\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "df_stemmed = df_lemm.withColumn(\"words_stemmed\", stemmer_udf(\"lemmi\")).select('IDX',\"overall\",'words_stemmed')\n",
    "\n",
    "\n",
    "# Filter length word > 3\n",
    "filter_length_udf = udf(lambda row: [x for x in row if len(x) > 3], ArrayType(StringType()))\n",
    "df_final_words = df_stemmed.withColumn('words', filter_length_udf(col('words_stemmed'))).select('IDX',\"overall\", 'words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Dataframe\n",
    "\n",
    "Here is final dataframe after cleaning and it is showing just above 10 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|IDX|overall|words                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+---+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |1.0    |[alexa, control, light, tell, lifx, give, exampl, group, name, exact, group, name, request, shell, await, doesnt, recogn, name, skill, buggi, work, even, rest, alexa, uninstal, lifx, everyth]                                                                                                                                                                                                                                                                                                                                              |\n",
      "|1  |4.0    |[alexa, work, great, also, control, singl, bulb, moment, turn, onoff, chang, color, adjust, bright, easi, quick, said, expect, complic, bulb, hope, best, prepar, worst, right, specul, user, frustrat, might, stem, alexa, recogn, bulb, room, name, simplifi, bulb, name, lamp, list, live, room, group, within, lifx, address, either, categori, pretti, consist, turn, onoff, live, room, light, chang, lamp, light, color, live, room, light, like, tech, expect, grow, pain, earli, patient, skill, isnt, perfect, mean, decent, start]|\n",
      "|2  |1.0    |[weak, alexa, doesnt, even, recogn, name, lifx, wast, time, even, turn, light]                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|3  |2.0    |[control, bulb, echo]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|4  |1.0    |[work, great, random, stop, pleas, updat]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|5  |5.0    |[great, skill]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|6  |1.0    |[pretti, crappi, wonrsquot, connect, alexi]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|7  |1.0    |[happi, connect, alexa, regardless]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|8  |1.0    |[connect, light, alexa, link, lifx, amazon, alexa, locat, smart, bulb, hard, connect, alexa, even, watch, tube, video, still]                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|9  |1.0    |[servic, work, googl, home, doesnt, work, alexa, sure, machin]                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+---+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final_words.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', \"i'll\", \"you'll\", \"he'll\", \"she'll\", \"we'll\", \"they'll\", \"i'd\", \"you'd\", \"he'd\", \"she'd\", \"we'd\", \"they'd\", \"i'm\", \"you're\", \"he's\", \"she's\", \"it's\", \"we're\", \"they're\", \"i've\", \"we've\", \"you've\", \"they've\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"haven't\", \"hasn't\", \"hadn't\", \"don't\", \"doesn't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"mustn't\", \"can't\", \"couldn't\", 'cannot', 'could', \"here's\", \"how's\", \"let's\", 'ought', \"that's\", \"there's\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", 'would']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "# Define a list of stop words or use default list\n",
    "remover = StopWordsRemover()\n",
    "stopwords = remover.getStopWords() # Display default list\n",
    "print(stopwords[:200])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
